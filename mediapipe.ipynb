{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python import BaseOptions\n",
    "from mediapipe.tasks.python.vision import GestureRecognizer, GestureRecognizerOptions, GestureRecognizerResult, RunningMode\n",
    "import cv2 \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from dm_control import mujoco\n",
    "from dm_control.rl import control\n",
    "from dm_control.suite import base\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "import matplotlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import *\n",
    "\n",
    "import pyrealsense2 as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimTask(base.Task):\n",
    "\n",
    "    def __init__(self, random=None):\n",
    "        super(SimTask, self).__init__(random=random)\n",
    "\n",
    "    def initialize_episode(self, physics):\n",
    "        with physics.reset_context():\n",
    "            physics.named.data.qpos[:6] = UR10E_START_POS\n",
    "            np.copyto(physics.data.ctrl, UR10E_START_POS)\n",
    "        \n",
    "        np.copyto(physics.data.mocap_pos[0], physics.named.data.xpos['wrist_3_link'])\n",
    "        np.copyto(physics.data.mocap_quat[0], physics.named.data.xquat['wrist_3_link'])\n",
    "\n",
    "        super().initialize_episode(physics)\n",
    "\n",
    "    def before_step(self, action, physics):\n",
    "        np.copyto(physics.data.mocap_pos[0], action[0:3])\n",
    "        np.copyto(physics.data.mocap_quat[0], action[3:7])\n",
    "    \n",
    "    def get_observation(self, physics):\n",
    "        return {\n",
    "            'images': {\n",
    "                'top': physics.render(height=240, width=320, camera_id=0),\n",
    "                'birdview': physics.render(height=240, width=320, camera_id=1),\n",
    "                'front': physics.render(height=240, width=320, camera_id=2)\n",
    "            }, 'mocap': {\n",
    "                'pos': physics.data.mocap_pos[0],\n",
    "                'quat': physics.data.mocap_quat[0]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_reward(self, physics):\n",
    "        return 0\n",
    "    \n",
    "    def get_joint_poses(self, physics):\n",
    "        # Get the joint positions from the Mujoco physics simulation\n",
    "        joint_poses = physics.named.data.qpos[6:]\n",
    "        return joint_poses\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_env(control_timestamp_sec=0.02):\n",
    "        physics = mujoco.Physics.from_xml_path(SCENE_XML_PATH)\n",
    "\n",
    "        task = SimTask()\n",
    "        env = control.Environment(\n",
    "            physics, task, time_limit=10., control_timestep=control_timestamp_sec,\n",
    "            n_sub_steps=None, flat_observation=None)\n",
    "        return env\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandTrackingSession:\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_frames=100, \n",
    "                 framerate=30,\n",
    "                 control_scale=1,\n",
    "                 control_timestep=0.002,\n",
    "                 render_robot=False) -> None:\n",
    "\n",
    "        self.pipeline = rs.pipeline()\n",
    "        config = rs.config()\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.rgb8, framerate)\n",
    "        self.pipeline.start(config)\n",
    "\n",
    "        self.camera_display_handle = None\n",
    "        self.simulation_display_handle = None\n",
    "\n",
    "        self.env = SimTask.get_env(control_timestamp_sec=control_timestep)\n",
    "        self.num_frames = num_frames\n",
    "        self.framerate = framerate\n",
    "\n",
    "        self.control_scale = control_scale\n",
    "\n",
    "        self.render_robot = render_robot\n",
    "\n",
    "        self.controls = []\n",
    "        self.camera_frames = []\n",
    "\n",
    "        self.simulation_frames = []\n",
    "\n",
    "        self.init_mocap_pos = None\n",
    "        self.init_mocap_quat = None\n",
    "\n",
    "\n",
    "    def callback(self, result: GestureRecognizerResult, output_image: mp.Image, timestamp: int):\n",
    "        '''\n",
    "        Async callback function for the gesture recognizer\n",
    "        '''\n",
    "        if (len(result.hand_landmarks) > 0):\n",
    "            landmark = result.hand_landmarks[0][0]\n",
    "            hand_pos = np.array([landmark.x, landmark.z, landmark.y])\n",
    "            print(hand_pos, end='\\r')\n",
    "        \n",
    "            self.controls.append(hand_pos)\n",
    "        else:\n",
    "            if (len(self.controls) > 0):\n",
    "                self.controls.append(self.controls[-1])\n",
    "            else:\n",
    "                self.controls.append(np.zeros(3))\n",
    "        \n",
    "        camera_frame = output_image.numpy_view()\n",
    "        self.camera_frames.append(camera_frame)\n",
    "        \n",
    "        self.camera_display_handle.update(PILImage.fromarray(camera_frame))\n",
    "\n",
    "    @staticmethod\n",
    "    def display_video(frames, framerate=30):\n",
    "        height, width, _ = frames[0].shape\n",
    "        dpi = 70\n",
    "        orig_backend = matplotlib.get_backend()\n",
    "        matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "        matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "        ax.set_axis_off()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_position([0, 0, 1, 1])\n",
    "        im = ax.imshow(frames[0])\n",
    "        def update(frame):\n",
    "            im.set_data(frame)\n",
    "            return [im]\n",
    "        interval = 2000/framerate\n",
    "        anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                        interval=interval, blit=True, repeat=False)\n",
    "        return HTML(anim.to_html5_video())\n",
    "    \n",
    "    def setup_env(self):\n",
    "        ts = self.env.reset()\n",
    "        self.init_mocap_pos = ts.observation['mocap']['pos'].copy()\n",
    "        self.init_mocap_quat = ts.observation['mocap']['quat'].copy()\n",
    "        self.simulation_frames = [ts.observation['images']['birdview']]\n",
    "    \n",
    "    def render_one_frame(self, ctrl_pos):\n",
    "        ts = self.env.step(np.concatenate([ctrl_pos * self.control_scale + self.init_mocap_pos, \n",
    "                                           self.init_mocap_quat]) \n",
    "                           )\n",
    "        return ts.observation['images']['birdview']\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        self.setup_env()\n",
    "\n",
    "        joint_poses_sequence = []  # Initialize list to store joint poses\n",
    "        \n",
    "        for i in tqdm(range(self.num_frames)):\n",
    "            # Render one frame and append to the simulation frames\n",
    "            self.simulation_frames.append(self.render_one_frame(self.controls[i]))\n",
    "            print(self.env.task.get_joint_poses(self.env.physics))\n",
    "            # Get the current joint poses and append to the sequence\n",
    "            current_joint_poses = self.env.task.get_joint_poses(self.env.physics)  # You need to implement this method in your SimTask class\n",
    "            joint_poses_sequence.append(current_joint_poses)\n",
    " \n",
    "        # Return both the rendered frames and the joint poses sequence\n",
    "        return self.display_video(self.simulation_frames, framerate=self.framerate)\n",
    "    \n",
    "    def start(self):\n",
    "        self.controls = []\n",
    "        self.camera_frames = []\n",
    "\n",
    "        self.camera_display_handle = display(None, display_id=True)\n",
    "\n",
    "        options = GestureRecognizerOptions(\n",
    "            base_options=BaseOptions(model_asset_path=MODEL_ASSET_PATH),\n",
    "            running_mode=RunningMode.LIVE_STREAM,\n",
    "            result_callback=self.callback\n",
    "            )\n",
    "        \n",
    "        timestamp = 0\n",
    "        with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "            while True:\n",
    "                frames = self.pipeline.wait_for_frames()\n",
    "                color_frame = frames.get_color_frame()\n",
    "                if not color_frame:\n",
    "                    continue\n",
    "                timestamp += 1\n",
    "                frame_data = np.asanyarray(color_frame.get_data())\n",
    "                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_data)\n",
    "                hand_landmarker_result = recognizer.recognize_async(mp_image, timestamp)\n",
    "\n",
    "                cv2.imshow('RealSense', frame_data)\n",
    "\n",
    "                if cv2.waitKey(1000 // self.framerate) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "                if timestamp == self.num_frames + 100: # 30 seconds\n",
    "                    break\n",
    "\n",
    "    def close(self):\n",
    "        self.pipeline.stop()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = HandTrackingSession(framerate=60, render_robot=False)\n",
    "session.start()\n",
    "\n",
    "session.render()\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aloha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
